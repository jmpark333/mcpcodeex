#!/usr/bin/env python3
"""
Anthropic ê³µì‹ MCP ê°œë… ì„¤ëª… ë°ëª¨
https://www.anthropic.com/engineering/code-execution-with-mcp

ì´ ì½”ë“œëŠ” Anthropicì´ ì„¤ëª…í•˜ëŠ” MCPì˜ í•µì‹¬ ê°œë…ì„ ì‹¤ì œë¡œ ë³´ì—¬ì¤ë‹ˆë‹¤:
1. ì ì§„ì  ê³µê°œ (Progressive Disclosure)
2. ìƒíƒœ ì €ì¥ (State Persistence) 
3. ì»¨í…ìŠ¤íŠ¸ íš¨ìœ¨ì„± (Context Efficiency)
"""

import asyncio
import json
import hashlib
import time
from pathlib import Path
from typing import Dict, Any, List, Optional

class AnthropicMCPConceptDemo:
    """Anthropic MCP ê°œë… ì‹¤ì œ ë°ëª¨"""
    
    def __init__(self, work_dir: str):
        self.work_dir = Path(work_dir)
        self.state_cache = {}  # MCPì˜ í•µì‹¬: ìƒíƒœ ì €ì¥
        self.execution_history = []  # ì‹¤í–‰ ê¸°ë¡
        
    async def demonstrate_progressive_disclosure(self):
        """1. ì ì§„ì  ê³µê°œ (Progressive Disclosure) ë°ëª¨"""
        print("ğŸ¯ 1. ì ì§„ì  ê³µê°œ (Progressive Disclosure)")
        print("=" * 60)
        print("MCPì˜ í•µì‹¬: í•„ìš”í•  ë•Œë§Œ ë„êµ¬ë¥¼ í˜¸ì¶œí•˜ì—¬ í† í° ì ˆì•½")
        print()
        
        # ì‹œë‚˜ë¦¬ì˜¤: ì‚¬ìš©ìê°€ ë¬¸ì„œë¥¼ ì°¾ê³  ì‹¶ì–´í•¨
        user_query = "AI ê¸°ìˆ  ê´€ë ¨ ë¬¸ì„œ ì°¾ì•„ì¤˜"
        
        # âŒ ê¸°ì¡´ ë°©ì‹: ì „ì²´ ë¬¸ì„œë¥¼ í•œ ë²ˆì— ë¡œë“œ
        print("âŒ ê¸°ì¡´ ë°©ì‹ (ë¹„íš¨ìœ¨ì ):")
        all_files = list(self.work_dir.glob("*.txt"))
        total_content = ""
        for file_path in all_files:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                total_content += f"\n\n=== {file_path.name} ===\n{content}"
        
        print(f"   ğŸ“Š ì „ì²´ {len(all_files)}ê°œ íŒŒì¼ ë¡œë“œ")
        print(f"   ğŸ“ ì´ {len(total_content):,} ì")
        print(f"   ğŸ’° í† í° ì‚¬ìš©ëŸ‰: ~{len(total_content) // 4:,} í† í° (1í† í°=4ì)")
        print()
        
        # âœ… MCP ë°©ì‹: ì ì§„ì ìœ¼ë¡œ í•„ìš”í•œ ê²ƒë§Œ ìš”ì²­
        print("âœ… MCP ë°©ì‹ (íš¨ìœ¨ì ):")
        
        # 1ë‹¨ê³„: ë„êµ¬ ëª©ë¡ë§Œ ë¨¼ì € í™•ì¸
        tools_response = await self._get_tools_list()
        print(f"   ğŸ”§ ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬: {len(tools_response['tools'])}ê°œ")
        print(f"   ğŸ’° í† í° ì‚¬ìš©ëŸ‰: ~{len(json.dumps(tools_response)) // 4} í† í°")
        print()
        
        # 2ë‹¨ê³„: ì‹¤ì œë¡œ í•„ìš”í•œ ë„êµ¬ë§Œ í˜¸ì¶œ
        search_response = await self._call_tool("search_files", {
            "query": "AI ê¸°ìˆ ",
            "max_results": 3
        })
        print(f"   ğŸ” ê²€ìƒ‰ ê²°ê³¼: {search_response['summary']}")
        print(f"   ğŸ’° í† í° ì‚¬ìš©ëŸ‰: ~{len(json.dumps(search_response)) // 4} í† í°")
        print()
        
        # 3ë‹¨ê³„: ê²°ê³¼ ë¶„ì„
        read_response = None  # ë³€ìˆ˜ ì´ˆê¸°í™”
        if search_response.get("results"):
            first_file = search_response["results"][0]
            read_response = await self._call_tool("read_file", {
                "path": first_file["name"]
            })
            print(f"   ğŸ“– íŒŒì¼ ì½ê¸°: {first_file['name']}")
            print(f"   ğŸ’° í† í° ì‚¬ìš©ëŸ‰: ~{len(json.dumps(read_response)) // 4} í† í°")
        
        print("\nğŸ“Š íš¨ìœ¨ì„± ë¹„êµ:")
        print(f"   ê¸°ì¡´ ë°©ì‹: ~{len(total_content) // 4:,} í† í°")
        
        # read_responseê°€ Noneì¸ ê²½ìš° ì²˜ë¦¬
        read_tokens = len(json.dumps(read_response)) // 4 if read_response else 0
        mcp_tokens = (len(json.dumps(tools_response)) + len(json.dumps(search_response)) + read_tokens) // 4
        print(f"   MCP ë°©ì‹: ~{mcp_tokens:,} í† í°")
        print(f"   ğŸ‰ í† í° ì ˆì•½: {((len(total_content) // 4) - mcp_tokens) / (len(total_content) // 4) * 100:.1f}%")
        
    async def demonstrate_state_persistence(self):
        """2. ìƒíƒœ ì €ì¥ (State Persistence) ë°ëª¨"""
        print("\nğŸ”„ 2. ìƒíƒœ ì €ì¥ (State Persistence)")
        print("=" * 60)
        print("MCPì˜ í•µì‹¬: ì´ì „ ì‹¤í–‰ ê²°ê³¼ë¥¼ ì €ì¥í•˜ì—¬ ì¬ì‚¬ìš©")
        print()
        
        # ì²« ë²ˆì§¸ ìš”ì²­
        print("ğŸ” ì²« ë²ˆì§¸ ìš”ì²­:")
        start_time = time.time()
        result1 = await self._call_tool_with_cache("search_files", {
            "query": "ë¨¸ì‹ ëŸ¬ë‹",
            "max_results": 5
        })
        first_time = time.time() - start_time
        print(f"   â±ï¸  ì‹¤í–‰ ì‹œê°„: {first_time:.3f}ì´ˆ")
        print(f"   ğŸ“Š ê²°ê³¼: {result1['summary']}")
        print(f"   ğŸ’¾ ìƒíƒœ ì €ì¥: {len(self.state_cache)}ê°œ í•­ëª©")
        print()
        
        # ë‘ ë²ˆì§¸ ë™ì¼ ìš”ì²­ (ìºì‹œ íˆíŠ¸)
        print("ğŸ”„ ë‘ ë²ˆì§¸ ë™ì¼ ìš”ì²­ (ìºì‹œ í…ŒìŠ¤íŠ¸):")
        start_time = time.time()
        result2 = await self._call_tool_with_cache("search_files", {
            "query": "ë¨¸ì‹ ëŸ¬ë‹", 
            "max_results": 5
        })
        second_time = time.time() - start_time
        print(f"   âš¡ ì‹¤í–‰ ì‹œê°„: {second_time:.3f}ì´ˆ")
        print(f"   ğŸ“Š ê²°ê³¼: {result2['summary']}")
        print(f"   ğŸ¯ ìºì‹œ íˆíŠ¸: ë™ì¼ ê²°ê³¼ ë°˜í™˜")
        print()
        
        print("ğŸ“ˆ ì„±ëŠ¥ í–¥ìƒ:")
        print(f"   ì†ë„ í–¥ìƒ: {(first_time / second_time):.1f}ë°° ë¹ ë¦„")
        print(f"   ì²˜ë¦¬ëŸ‰ ì ˆì•½: 100% (ë°ì´í„°ë² ì´ìŠ¤ ì¡°íšŒ ë¶ˆí•„ìš”)")
        
    async def demonstrate_context_efficiency(self):
        """3. ì»¨í…ìŠ¤íŠ¸ íš¨ìœ¨ì„± (Context Efficiency) ë°ëª¨"""
        print("\nğŸ¯ 3. ì»¨í…ìŠ¤íŠ¸ íš¨ìœ¨ì„± (Context Efficiency)")
        print("=" * 60)
        print("MCPì˜ í•µì‹¬: í•„í„°ë§ëœ ë°ì´í„°ë§Œ ì»¨í…ìŠ¤íŠ¸ì— í¬í•¨")
        print()
        
        # ëŒ€ìš©ëŸ‰ ë°ì´í„° ì‹œë®¬ë ˆì´ì…˜
        large_dataset = []
        for i in range(100):  # 100ê°œì˜ ëŒ€ìš©ëŸ‰ ë¬¸ì„œ
            doc = {
                "id": f"doc_{i:03d}",
                "title": f"ê¸°ìˆ  ë¬¸ì„œ {i}",
                "content": f"ì´ê²ƒì€ {i}ë²ˆì§¸ ê¸°ìˆ  ë¬¸ì„œì…ë‹ˆë‹¤. " + "A" * 1000,
                "category": ["AI", "ML", "DL", "NLP", "CV"][i % 5],
                "size": 1000 + i * 10
            }
            large_dataset.append(doc)
        
        print(f"ğŸ“š ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹: {len(large_dataset)}ê°œ ë¬¸ì„œ")
        print(f"   ğŸ“ ì´ í¬ê¸°: {sum(doc['size'] for doc in large_dataset):,} ë°”ì´íŠ¸")
        print()
        
        # âŒ ê¸°ì¡´ ë°©ì‹: ì „ì²´ ë°ì´í„°ë¥¼ ì»¨í…ìŠ¤íŠ¸ì— í¬í•¨
        print("âŒ ê¸°ì¡´ ë°©ì‹:")
        context_size_old = len(json.dumps(large_dataset))
        print(f"   ğŸ“ ì»¨í…ìŠ¤íŠ¸ í¬ê¸°: {context_size_old:,} ì")
        print(f"   ğŸ’° í† í° ì‚¬ìš©ëŸ‰: ~{context_size_old // 4:,} í† í°")
        print(f"   âš ï¸  ë¬¸ì œ: ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ì´ˆê³¼ ê°€ëŠ¥ì„±")
        print()
        
        # âœ… MCP ë°©ì‹: í•„í„°ë§ í›„ ê´€ë ¨ ë°ì´í„°ë§Œ í¬í•¨
        print("âœ… MCP ë°©ì‹:")
        
        # 1ë‹¨ê³„: ë©”íƒ€ë°ì´í„°ë§Œ ë¨¼ì € ì¡°íšŒ
        metadata_response = await self._call_tool("get_metadata", {
            "category": "AI",
            "limit": 10
        })
        
        # 2ë‹¨ê³„: ê´€ë ¨ ë¬¸ì„œ IDë§Œ í™•ë³´
        if metadata_response.get("document_ids"):
            doc_ids = metadata_response["document_ids"]
            filtered_response = await self._call_tool("get_documents", {
                "document_ids": doc_ids,
                "fields": ["title", "summary"]  # í•„ìš”í•œ í•„ë“œë§Œ
            })
            
            context_size_new = len(json.dumps(filtered_response))
            print(f"   ğŸ“ ì»¨í…ìŠ¤íŠ¸ í¬ê¸°: {context_size_new:,} ì")
            print(f"   ğŸ’° í† í° ì‚¬ìš©ëŸ‰: ~{context_size_new // 4:,} í† í°")
            print(f"   ğŸ¯ í•„í„°ë§: {len(doc_ids)}ê°œ ë¬¸ì„œë§Œ ì„ íƒ")
            print()
            
            print("ğŸ“Š íš¨ìœ¨ì„± ë¹„êµ:")
            reduction = ((context_size_old - context_size_new) / context_size_old) * 100
            print(f"   ì»¨í…ìŠ¤íŠ¸ ê°ì†Œ: {reduction:.1f}%")
            print(f"   í† í° ì ˆì•½: {(context_size_old // 4) - (context_size_new // 4):,} í† í°")
            print(f"   ğŸ¯ ëª©í‘œ ë‹¬ì„±: ê´€ë ¨ ì •ë³´ë§Œ ì •í™•íˆ ì „ë‹¬")
        
    async def _get_tools_list(self) -> Dict[str, Any]:
        """ë„êµ¬ ëª©ë¡ ì¡°íšŒ (MCP í‘œì¤€)"""
        return {
            "tools": [
                {
                    "name": "search_files",
                    "description": "íŒŒì¼ ê²€ìƒ‰",
                    "inputSchema": {"type": "object", "properties": {"query": {"type": "string"}}}
                },
                {
                    "name": "read_file", 
                    "description": "íŒŒì¼ ì½ê¸°",
                    "inputSchema": {"type": "object", "properties": {"path": {"type": "string"}}}
                },
                {
                    "name": "get_metadata",
                    "description": "ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ì¡°íšŒ",
                    "inputSchema": {"type": "object", "properties": {"category": {"type": "string"}}}
                },
                {
                    "name": "get_documents",
                    "description": "íŠ¹ì • ë¬¸ì„œ ì¡°íšŒ",
                    "inputSchema": {"type": "object", "properties": {"document_ids": {"type": "array"}}}
                }
            ]
        }
    
    async def _call_tool(self, tool_name: str, arguments: Dict[str, Any]) -> Dict[str, Any]:
        """ë„êµ¬ í˜¸ì¶œ ì‹œë®¬ë ˆì´ì…˜"""
        print(f"   ğŸ”§ ë„êµ¬ í˜¸ì¶œ: {tool_name}")
        print(f"   ğŸ“¥ íŒŒë¼ë¯¸í„°: {arguments}")
        
        if tool_name == "search_files":
            # ì‹¤ì œ íŒŒì¼ ê²€ìƒ‰
            results = []
            query = arguments.get("query", "").lower()
            max_results = arguments.get("max_results", 10)
            
            for file_path in self.work_dir.glob("*.txt"):
                if query in file_path.name.lower():
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                        results.append({
                            "name": file_path.name,
                            "path": str(file_path),
                            "size": len(content),
                            "preview": content[:100] + "..." if len(content) > 100 else content
                        })
                        if len(results) >= max_results:
                            break
            
            return {
                "summary": f"Found {len(results)} files matching '{query}'",
                "results": results
            }
            
        elif tool_name == "read_file":
            path = self.work_dir / arguments["path"]
            with open(path, 'r', encoding='utf-8') as f:
                content = f.read()
            return {
                "path": arguments["path"],
                "content": content,
                "size": len(content)
            }
            
        elif tool_name == "get_metadata":
            # ë©”íƒ€ë°ì´í„° ì‹œë®¬ë ˆì´ì…˜
            category = arguments.get("category", "")
            return {
                "document_ids": [f"doc_{i:03d}" for i in range(10)],
                "category": category,
                "total_count": 100
            }
            
        elif tool_name == "get_documents":
            # í•„í„°ë§ëœ ë¬¸ì„œ ì‹œë®¬ë ˆì´ì…˜
            doc_ids = arguments.get("document_ids", [])
            fields = arguments.get("fields", ["title", "summary"])
            
            documents = []
            for doc_id in doc_ids:
                doc_data = {field: f"{doc_id}_{field}" for field in fields}
                documents.append(doc_data)
                
            return {
                "documents": documents,
                "fields": fields,
                "count": len(documents)
            }
        
        return {"error": f"Unknown tool: {tool_name}"}
    
    async def _call_tool_with_cache(self, tool_name: str, arguments: Dict[str, Any]) -> Dict[str, Any]:
        """ìºì‹œë¥¼ í¬í•¨í•œ ë„êµ¬ í˜¸ì¶œ"""
        # ìºì‹œ í‚¤ ìƒì„±
        cache_key = f"{tool_name}_{hashlib.md5(json.dumps(arguments, sort_keys=True).encode()).hexdigest()}"
        
        # ìºì‹œ í™•ì¸
        if cache_key in self.state_cache:
            cached_entry = self.state_cache[cache_key]
            if time.time() - cached_entry["timestamp"] < 300:  # 5ë¶„ TTL
                print(f"   ğŸ¯ ìºì‹œ íˆíŠ¸: {tool_name}")
                cached_entry["hit_count"] += 1
                return cached_entry["result"]
        
        # ìºì‹œ ë¯¸ìŠ¤ - ì‹¤ì œ ì‹¤í–‰
        print(f"   ğŸ” ìºì‹œ ë¯¸ìŠ¤: {tool_name} ì‹¤í–‰")
        result = await self._call_tool(tool_name, arguments)
        
        # ê²°ê³¼ ì €ì¥
        self.state_cache[cache_key] = {
            "result": result,
            "timestamp": time.time(),
            "hit_count": 0,
            "tool_name": tool_name,
            "arguments": arguments
        }
        
        return result


async def main():
    """Anthropic MCP ê°œë… ë°ëª¨ ì‹¤í–‰"""
    print("ğŸ¤– Anthropic ê³µì‹ MCP ê°œë… ë°ëª¨")
    print("https://www.anthropic.com/engineering/code-execution-with-mcp")
    print("=" * 80)
    
    demo = AnthropicMCPConceptDemo("mcp_workspace")
    
    try:
        # 1. ì ì§„ì  ê³µê°œ ë°ëª¨
        await demo.demonstrate_progressive_disclosure()
        
        # 2. ìƒíƒœ ì €ì¥ ë°ëª¨
        await demo.demonstrate_state_persistence()
        
        # 3. ì»¨í…ìŠ¤íŠ¸ íš¨ìœ¨ì„± ë°ëª¨
        await demo.demonstrate_context_efficiency()
        
        print("\nğŸ‰ Anthropic MCP ê°œë… ë°ëª¨ ì™„ë£Œ!")
        print("=" * 60)
        print("ğŸ’¡ MCPì˜ í•µì‹¬ ê°€ì¹˜:")
        print("   1. ì ì§„ì  ê³µê°œ: í•„ìš”í•  ë•Œë§Œ ë„êµ¬ í˜¸ì¶œ")
        print("   2. ìƒíƒœ ì €ì¥: ì´ì „ ê²°ê³¼ ì¬ì‚¬ìš©")
        print("   3. ì»¨í…ìŠ¤íŠ¸ íš¨ìœ¨ì„±: í•„í„°ë§ëœ ë°ì´í„°ë§Œ ì²˜ë¦¬")
        print("   4. ì‹¤ì œ ìƒí˜¸ì‘ìš©: íŒŒì¼ ì‹œìŠ¤í…œê³¼ ì§ì ‘ í†µì‹ ")
        print("   5. í† í° íš¨ìœ¨ì„±: 90% ì´ìƒ ì ˆì•½ ê°€ëŠ¥")
        
    except Exception as e:
        print(f"âŒ ë°ëª¨ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")


if __name__ == "__main__":
    print("ğŸš€ Anthropic MCP ê°œë… ì‹¤ì œ ë°ëª¨")
    print("ì´ê²ƒì€ Anthropicì´ ì„¤ëª…í•˜ëŠ” MCPì˜ í•µì‹¬ ì›ë¦¬ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤!")
    print()
    
    asyncio.run(main())
