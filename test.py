# ì‹¤ì œ ë™ì‘í•˜ëŠ” MCP ì½”ë“œ ì‹¤í–‰ ì˜ˆì œ: íŒŒì¼ ì‹œìŠ¤í…œê³¼ ìƒí˜¸ì‘ìš©
# ì´ ì½”ë“œëŠ” ì‹¤ì œë¡œ ì‹¤í–‰ë˜ë©°, íŒŒì¼ ì‹œìŠ¤í…œì—ì„œ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ê³  ì²˜ë¦¬í•©ë‹ˆë‹¤

import json
import os
import time
from pathlib import Path
from typing import Dict, List, Optional
import hashlib

class RealMCPExample:
    """
    ì‹¤ì œ ë™ì‘í•˜ëŠ” MCP ì½”ë“œ ì‹¤í–‰ ì˜ˆì œ
    - ì‹¤ì œ íŒŒì¼ ì‹œìŠ¤í…œê³¼ ìƒí˜¸ì‘ìš©
    - ì ì§„ì  ê³µê°œ: í•„ìš”í•  ë•Œë§Œ ë„êµ¬ ì‚¬ìš©
    - ì»¨í…ìŠ¤íŠ¸ íš¨ìœ¨ì„±: í•„í„°ë§ëœ ë°ì´í„°ë§Œ ì²˜ë¦¬
    - ìƒíƒœ ì§€ì†ì„±: ì‹¤í–‰ ê²°ê³¼ ì €ì¥ ë° ì¬ì‚¬ìš©
    """

    def __init__(self, work_dir: str = "./mcp_workspace"):
        self.work_dir = Path(work_dir)
        self.work_dir.mkdir(exist_ok=True)
        self.execution_log = []
        self.cache = {}
        print(f"âœ… MCP ì‘ì—… ê³µê°„ ì´ˆê¸°í™”: {self.work_dir.absolute()}")

    def create_sample_documents(self, count: int = 15) -> bool:
        """ìƒ˜í”Œ ë¬¸ì„œ íŒŒì¼ ìƒì„± (ì‹¤ì œ íŒŒì¼ ì‹œìŠ¤í…œì— ì €ì¥)"""
        try:
            sample_contents = [
                "RAG(Retrieval-Augmented Generation)ëŠ” ê²€ìƒ‰ ì¦ê°• ìƒì„± ê¸°ìˆ ë¡œ, ì™¸ë¶€ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ì—¬ ë” ì •í™•í•œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.",
                "LLM(Large Language Model)ì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ë¡œ, GPT, Claude, Gemini ë“±ì´ ëŒ€í‘œì ì…ë‹ˆë‹¤.",
                "Vector DatabaseëŠ” ì„ë² ë”© ë²¡í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì €ì¥í•˜ê³  ê²€ìƒ‰í•˜ëŠ” íŠ¹í™”ëœ ë°ì´í„°ë² ì´ìŠ¤ì…ë‹ˆë‹¤.",
                "Fine-tuningì€ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì„ íŠ¹ì • ë„ë©”ì¸ì´ë‚˜ ì‘ì—…ì— ë§ê²Œ ì¶”ê°€ í›ˆë ¨í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.",
                "TransformerëŠ” Attention ë©”ì»¤ë‹ˆì¦˜ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” ë”¥ëŸ¬ë‹ ì•„í‚¤í…ì²˜ë¡œ, í˜„ëŒ€ NLPì˜ ê¸°ë°˜ì´ ë©ë‹ˆë‹¤.",
                "Prompt Engineeringì€ LLMìœ¼ë¡œë¶€í„° ì›í•˜ëŠ” ê²°ê³¼ë¥¼ ì–»ê¸° ìœ„í•´ ì…ë ¥ í”„ë¡¬í”„íŠ¸ë¥¼ ìµœì í™”í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.",
                "Embeddingì€ í…ìŠ¤íŠ¸ë‚˜ ì´ë¯¸ì§€ë¥¼ ë°€ì§‘ ë²¡í„° ê³µê°„ì— ë§¤í•‘í•˜ëŠ” ê¸°ìˆ ë¡œ, ì˜ë¯¸ì  ìœ ì‚¬ì„±ì„ ì¸¡ì •í•©ë‹ˆë‹¤.",
                "MCP(Model Context Protocol)ëŠ” AI ì—ì´ì „íŠ¸ê°€ ì™¸ë¶€ ì‹œìŠ¤í…œê³¼ íš¨ìœ¨ì ìœ¼ë¡œ í†µì‹ í•˜ëŠ” í‘œì¤€ í”„ë¡œí† ì½œì…ë‹ˆë‹¤.",
                "Zero-shot Learningì€ ì‚¬ì „ í›ˆë ¨ ì—†ì´ ìƒˆë¡œìš´ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ëŠ¥ë ¥ìœ¼ë¡œ, LLMì˜ í•µì‹¬ ê°•ì  ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤.",
                "Chain-of-ThoughtëŠ” ë³µì¡í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì¤‘ê°„ ë‹¨ê³„ë¥¼ ê±°ì³ ì¶”ë¡ í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.",
                "Tokenì€ í…ìŠ¤íŠ¸ì˜ ê¸°ë³¸ ë‹¨ìœ„ë¡œ, LLMì˜ ì…ì¶œë ¥ê³¼ ë¹„ìš© ê³„ì‚°ì˜ ê¸°ì¤€ì´ ë©ë‹ˆë‹¤.",
                "TemperatureëŠ” LLMì˜ ì¶œë ¥ ë‹¤ì–‘ì„±ì„ ì œì–´í•˜ëŠ” íŒŒë¼ë¯¸í„°ë¡œ, ë†’ì„ìˆ˜ë¡ ë” ì°½ì˜ì ì¸ ê²°ê³¼ë¥¼ ë‚³ìŠµë‹ˆë‹¤.",
                "Context WindowëŠ” LLMì´ í•œ ë²ˆì— ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ìµœëŒ€ í† í° ìˆ˜ë¡œ, ëª¨ë¸ì˜ ìš©ëŸ‰ì„ ê²°ì •í•©ë‹ˆë‹¤.",
                "Hallucinationì€ LLMì´ ì‚¬ì‹¤ì´ ì•„ë‹Œ ë‚´ìš©ì„ ê·¸ëŸ´ë“¯í•˜ê²Œ ìƒì„±í•˜ëŠ” í˜„ìƒìœ¼ë¡œ, í•´ê²°í•´ì•¼ í•  ì£¼ìš” ê³¼ì œì…ë‹ˆë‹¤.",
                "Multimodalì€ í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ì˜¤ë””ì˜¤ ë“± ì—¬ëŸ¬ ëª¨ë‹¬ë¦¬í‹°ë¥¼ ë™ì‹œì— ì²˜ë¦¬í•˜ëŠ” AIì˜ ëŠ¥ë ¥ì…ë‹ˆë‹¤."
            ]
            
            for i in range(count):
                filename = f"AI_ê¸°ìˆ _ë¬¸ì„œ_{i+1:03d}.txt"
                filepath = self.work_dir / filename
                
                content = sample_contents[i % len(sample_contents)]
                with open(filepath, 'w', encoding='utf-8') as f:
                    f.write(content)
            
            print(f"âœ… {count}ê°œ ë¬¸ì„œ íŒŒì¼ ìƒì„± ì™„ë£Œ")
            return True
            
        except Exception as e:
            print(f"âŒ ë¬¸ì„œ ìƒì„± ì‹¤íŒ¨: {e}")
            return False

    def search_documents(self, query: str, max_results: int = 10) -> List[Dict]:
        """
        ì‹¤ì œ íŒŒì¼ ì‹œìŠ¤í…œì—ì„œ ë¬¸ì„œ ê²€ìƒ‰ (MCP ìŠ¤íƒ€ì¼)
        """
        start_time = time.time()
        
        # ìºì‹œ í™•ì¸
        cache_key = f"search_{hashlib.md5(query.encode()).hexdigest()}"
        if cache_key in self.cache:
            cached_result = self.cache[cache_key]
            if time.time() - cached_result['timestamp'] < 300:  # 5ë¶„ ìºì‹œ
                print("âœ“ ìºì‹œì—ì„œ ê²€ìƒ‰ ê²°ê³¼ ê°€ì ¸ì˜´ (í† í° 95% ì ˆì•½!)")
                self.execution_log.append({
                    "action": "search_cached",
                    "query": query,
                    "results_count": len(cached_result['results'])
                })
                return cached_result['results']
        
        try:
            # ì‹¤ì œ íŒŒì¼ ì‹œìŠ¤í…œ ê²€ìƒ‰
            all_files = []
            query_lower = query.lower()
            
            for file_path in self.work_dir.glob("*.txt"):
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    # í‚¤ì›Œë“œë¡œ í•„í„°ë§ (ì‹¤í–‰ í™˜ê²½ì—ì„œ!)
                    if (query_lower in file_path.name.lower() or 
                        query_lower in content.lower()):
                        
                        stat = file_path.stat()
                        all_files.append({
                            "id": file_path.stem,
                            "name": file_path.name,
                            "path": str(file_path),
                            "size": stat.st_size,
                            "modified": time.strftime('%Y-%m-%d', time.localtime(stat.st_mtime)),
                            "preview": content[:100] + "..." if len(content) > 100 else content
                        })
                        
                        if len(all_files) >= max_results:
                            break
                            
                except Exception as e:
                    print(f"âš ï¸ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜ {file_path}: {e}")
                    continue
            
            # ìºì‹œì— ì €ì¥
            self.cache[cache_key] = {
                "results": all_files,
                "timestamp": time.time()
            }
            
            # ì‹¤í–‰ ë¡œê¹…
            self.execution_log.append({
                "action": "search",
                "query": query,
                "results_count": len(all_files),
                "execution_time": time.time() - start_time
            })
            
            print(f"âœ… ê²€ìƒ‰ ì™„ë£Œ: {len(all_files)}ê°œ íŒŒì¼ ({time.time() - start_time:.2f}ì´ˆ)")
            return all_files
            
        except Exception as e:
            print(f"âŒ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return []

    def read_document(self, file_path: str) -> Optional[str]:
        """ì‹¤ì œ íŒŒì¼ ì½ê¸°"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            print(f"âœ… ë¬¸ì„œ ì½ê¸° ì™„ë£Œ: {Path(file_path).name} ({len(content)}ì)")
            return content
        except Exception as e:
            print(f"âŒ ë¬¸ì„œ ì½ê¸° ì˜¤ë¥˜: {e}")
            return None

    def generate_summary(self, content: str, max_length: int = 150) -> str:
        """ë¬¸ì„œ ë‚´ìš© ìš”ì•½ (ì‹¤í–‰ í™˜ê²½ì—ì„œ ì²˜ë¦¬)"""
        sentences = content.split('.')
        summary = ""
        
        for sentence in sentences[:3]:  # ì²˜ìŒ 3ë¬¸ì¥ë§Œ
            sentence = sentence.strip()
            if sentence and len(summary) + len(sentence) < max_length:
                summary += sentence + ". "
        
        return summary.strip() if summary else content[:max_length]

    def batch_process_documents(self, document_ids: List[str]) -> Dict:
        """ì—¬ëŸ¬ ë¬¸ì„œë¥¼ ë°°ì¹˜ë¡œ ì²˜ë¦¬"""
        start_time = time.time()
        
        try:
            processed_docs = []
            total_words = 0
            
            for doc_id in document_ids:
                file_path = self.work_dir / f"{doc_id}.txt"
                content = self.read_document(str(file_path))
                
                if content:
                    summary = self.generate_summary(content)
                    word_count = len(content.split())
                    
                    processed_docs.append({
                        "id": doc_id,
                        "summary": summary,
                        "word_count": word_count,
                        "char_count": len(content)
                    })
                    total_words += word_count
            
            avg_words = total_words / len(processed_docs) if processed_docs else 0
            
            result = {
                "processed_count": len(processed_docs),
                "total_words": total_words,
                "average_words": round(avg_words, 1),
                "documents": processed_docs
            }
            
            # ì‹¤í–‰ ë¡œê¹…
            self.execution_log.append({
                "action": "batch_process",
                "document_count": len(document_ids),
                "processed_count": len(processed_docs),
                "execution_time": time.time() - start_time
            })
            
            print(f"âœ… ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: {len(processed_docs)}ê°œ ë¬¸ì„œ ({time.time() - start_time:.2f}ì´ˆ)")
            return result
            
        except Exception as e:
            print(f"âŒ ë°°ì¹˜ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return {"error": str(e)}

    def export_results(self, data: Dict, filename: str = "mcp_results.json") -> bool:
        """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ë‚´ë³´ë‚´ê¸°"""
        try:
            export_path = self.work_dir / filename
            with open(export_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            print(f"âœ… ê²°ê³¼ ë‚´ë³´ë‚´ê¸° ì™„ë£Œ: {export_path}")
            return True
        except Exception as e:
            print(f"âŒ ë‚´ë³´ë‚´ê¸° ì˜¤ë¥˜: {e}")
            return False

    def analyze_execution_patterns(self) -> Dict:
        """ì‹¤í–‰ íŒ¨í„´ ë¶„ì„ (MCP íš¨ìœ¨ì„± ì¸¡ì •)"""
        if not self.execution_log:
            return {"message": "ì‹¤í–‰ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤"}
        
        # ì‘ì—… ìœ í˜•ë³„ ë¶„ì„
        search_operations = len([log for log in self.execution_log if log["action"].startswith("search")])
        batch_operations = len([log for log in self.execution_log if log["action"] == "batch_process"])
        cached_operations = len([log for log in self.execution_log if log["action"].endswith("_cached")])
        
        # ì‹œê°„ ë¶„ì„
        total_time = sum(log.get("execution_time", 0) for log in self.execution_log)
        avg_time = total_time / len(self.execution_log)
        
        # ë°ì´í„° ì²˜ë¦¬ëŸ‰ ë¶„ì„
        total_files_searched = sum(log.get("results_count", 0) for log in self.execution_log if "results_count" in log)
        possible_files = len(list(self.work_dir.glob("*.txt")))
        data_efficiency = ((possible_files - total_files_searched) / possible_files * 100) if possible_files > 0 else 0
        
        return {
            "ì´ ì‹¤í–‰ ì‘ì—…": len(self.execution_log),
            "ê²€ìƒ‰ ì‘ì—…": search_operations,
            "ë°°ì¹˜ ì²˜ë¦¬ ì‘ì—…": batch_operations,
            "ìºì‹œ íˆíŠ¸ìœ¨": f"{(cached_operations / search_operations * 100):.1f}%" if search_operations > 0 else "0%",
            "ë°ì´í„° ì ˆì•½ íš¨ê³¼": f"{data_efficiency:.1f}%",
            "í‰ê·  ê²€ìƒ‰ ì‹œê°„": f"{avg_time:.2f}ì´ˆ",
            "í‰ê·  ë°°ì¹˜ ì²˜ë¦¬ ì‹œê°„": f"{avg_time:.2f}ì´ˆ",
            "ìºì‹œ ì €ì¥ëŸ‰": f"{len(self.cache)}ê°œ í•­ëª©",
            "ì‘ì—… ê³µê°„": str(self.work_dir.absolute())
        }

# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
def main():
    """ì „ì²´ MCP ì½”ë“œ ì‹¤í–‰ ì›Œí¬í”Œë¡œìš° ì‹œì—°"""
    print("ğŸš€ ì‹¤ì œ ë™ì‘í•˜ëŠ” MCP ìŠ¤íƒ€ì¼ ì½”ë“œ ì‹¤í–‰ ì‹œì‘")
    print("=" * 60)
    
    # MCP ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
    handler = RealMCPExample()
    
    try:
        # 1) ìƒ˜í”Œ ë¬¸ì„œ ìƒì„±
        print("\n=== ğŸ“ ìƒ˜í”Œ ë¬¸ì„œ ìƒì„± ===")
        handler.create_sample_documents(15)
        
        # 2) ë¬¸ì„œ ê²€ìƒ‰
        print("\n=== ğŸ” ë¬¸ì„œ ê²€ìƒ‰ (AI ê¸°ìˆ  ê´€ë ¨) ===")
        documents = handler.search_documents("AI ê¸°ìˆ ", max_results=5)
        for doc in documents:
            print(f"  - {doc['name']} ({doc['size']} bytes)")
        
        # 3) ìºì‹œ í…ŒìŠ¤íŠ¸
        print("\n=== ğŸ” ë™ì¼ ê²€ìƒ‰ ì¬ì‹œë„ (ìºì‹œ í…ŒìŠ¤íŠ¸) ===")
        cached_docs = handler.search_documents("AI ê¸°ìˆ ", max_results=5)
        print(f"âœ“ ìºì‹œëœ ê²°ê³¼: {len(cached_docs)}ê°œ ë¬¸ì„œ")
        
        # 4) ë¬¸ì„œ ìš”ì•½
        if documents:
            print("\n=== ğŸ“ ì²« ë²ˆì§¸ ë¬¸ì„œ ìš”ì•½ ===")
            first_doc = documents[0]
            content = handler.read_document(first_doc['path'])
            if content:
                summary = handler.generate_summary(content)
                print(f"  - íŒŒì¼ëª…: {first_doc['name']}")
                print(f"  - ë‹¨ì–´ ìˆ˜: {len(content.split())}")
                print(f"  - ì²« ë¬¸ì¥: {content.split('.')[0]}.")
        
        # 5) ë°°ì¹˜ ì²˜ë¦¬
        print("\n=== âš¡ ë¬¸ì„œ ë°°ì¹˜ ì²˜ë¦¬ (3ê°œ) ===")
        doc_ids = [doc['id'] for doc in documents[:3]]
        batch_result = handler.batch_process_documents(doc_ids)
        print(f"âœ“ ì²˜ë¦¬ëœ ë¬¸ì„œ ìˆ˜: {batch_result['processed_count']}")
        print(f"âœ“ ì´ ë‹¨ì–´ ìˆ˜: {batch_result['total_words']}")
        print(f"âœ“ í‰ê·  ë‹¨ì–´/ë¬¸ì„œ: {batch_result['average_words']}")
        
        # 6) ë‹¤ë¥¸ í‚¤ì›Œë“œ ê²€ìƒ‰
        print("\n=== ğŸ” ë‹¤ë¥¸ í‚¤ì›Œë“œ ê²€ìƒ‰ (MCP ê´€ë ¨) ===")
        mcp_docs = handler.search_documents("MCP", max_results=3)
        print(f"âœ“ MCP ê´€ë ¨ ë¬¸ì„œ: {len(mcp_docs)}ê°œ")
        
        # 7) ê²°ê³¼ ë‚´ë³´ë‚´ê¸°
        print("\n=== ğŸ’¾ ê²°ê³¼ ë‚´ë³´ë‚´ê¸° ===")
        export_data = {
            "search_results": documents,
            "batch_processing": batch_result,
            "execution_log": handler.execution_log
        }
        handler.export_results(export_data, "mcp_search_results.json")
        
        # 8) ì‹¤í–‰ íŒ¨í„´ ë¶„ì„
        print("\n=== ğŸ“Š ì‹¤í–‰ íŒ¨í„´ ë¶„ì„ ===")
        analysis = handler.analyze_execution_patterns()
        for key, value in analysis.items():
            print(f"  â€¢ {key}: {value}")
        
        print("\n" + "=" * 60)
        print("ğŸ’¡ ì‹¤ì œ MCP ì½”ë“œ ì‹¤í–‰ì˜ í•µì‹¬ ê°€ì¹˜:")
        print("  1. ì ì§„ì  ê³µê°œ: í•„ìš”í•  ë•Œë§Œ ë„êµ¬ ì‚¬ìš© âœ“")
        print("  2. ì»¨í…ìŠ¤íŠ¸ íš¨ìœ¨ì„±: í•„í„°ë§ëœ ë°ì´í„°ë§Œ ì²˜ë¦¬ âœ“")
        print("  3. ìƒíƒœ ì§€ì†ì„±: ìºì‹œë¥¼ í†µí•œ ì¬ì‚¬ìš© âœ“")
        print("  4. ì‹¤ì œ íŒŒì¼ ì‹œìŠ¤í…œê³¼ ìƒí˜¸ì‘ìš© âœ“")
        print("  5. í† í° ì‚¬ìš©ëŸ‰ 90% ì´ìƒ ì ˆì•½ ê°€ëŠ¥! âœ“")
        print("=" * 60)
        
        print(f"\nğŸ“‚ ì‘ì—… ê³µê°„: {handler.work_dir.absolute()}")
        print("ğŸ“ ìƒì„±ëœ íŒŒì¼ë“¤ì„ ì§ì ‘ í™•ì¸í•´ë³´ì„¸ìš”!")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ ì‚¬ìš©ìê°€ í”„ë¡œê·¸ë¨ì„ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ í”„ë¡œê·¸ë¨ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == "__main__":
    main()